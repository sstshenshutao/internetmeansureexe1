{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "a) Read up on the OpenINTEL project on the website and in the respective publication2\n",
    "to briefly answer the\n",
    "following questions: What kinds of measurements are OpenINTEL performing? What are some challenges of\n",
    "the measurements? Which resource records are measured for each domain? For which labels/subdomains of\n",
    "the domains are the queries performed? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "OpenINTEL performs both forward and reverse DNS measurements.\n",
    "It performs active measurements (sends queries), so they have a consistent and reliable state of the global DNS.\n",
    "\n",
    "There are some challenges:\n",
    "\n",
    "A large-scale active measurement: daily collect\n",
    "data for the largest top level domain. \".com\" requires at least 1.85B queries per day.\n",
    "\n",
    "Large Storage: more than 240GB\n",
    "of results need to be stored per day for .com alone\n",
    "\n",
    "The program Robustness: it should run continuously, not crash\n",
    "\n",
    "The impacts of the global DNS infrastructure: this scale of active measurement should not impose an unacceptable\n",
    "burden, and so on\n",
    "\n",
    "SOA\n",
    "NS\n",
    "A\n",
    "AAAA\n",
    "MX\n",
    "TXT\n",
    "DNSKEY\n",
    "DS\n",
    "NSEC3\n",
    "CAA\n",
    "CDS\n",
    "CDNSKEY, these records are measured every second-level domain in a TLD, also for www label, non-existent domain name to record authenticated denial-of-existence data\n",
    ", A and AAAA records for these records in a separate infrastructure measurement, TLSA records for ports 25, 465 and 587 in a separate infrastructure measurement.\n",
    "\n",
    "Every second-level domain in a TLD and 'www' labels are the queries performed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b) Write a program that downloads DNS measurements for Alexa Top 1M and Cisco Umbrella from OpenINTEL3\n",
    "for a specified date. Extend your program so that it untars the files and opens the .avro files within, and inspect\n",
    "the file schema and format. Make your script extract all responses that correspond to A (IPv4 addresses), AAAA\n",
    "(IPv6 addresses), and CNAME resource records for each queried www. subdomain, and store the results (along\n",
    "with the measurement date) in a SQL database; distinguish between the Alexa and Umbrella list accordingly by\n",
    "creating separate tables. Finally, extend your script to repeat this process for measurements of a whole month\n",
    "(specified by the user). (2 points)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "solution b):\n",
    "\n",
    "The program is \"question_b.py\". To start it you should provide a multiprocessing environment.\n",
    "Note: it is recommended to start the program with max buffer size 10000\n",
    "\n",
    "An example is: **\"question_b.py alexa 2020 10 --buffer 10000\"**\n",
    "```\n",
    "usage: question_b.py [-h] [--day [day]] [--db [db_name]] [--cache [cache_dir]]\n",
    "                     [--buffer [buffer_max]]\n",
    "                     source year month\n",
    "\n",
    "question_b\n",
    "\n",
    "positional arguments:\n",
    "  source                the source of the data, can be Alexa or Umbrella\n",
    "  year                  the year to query, example: 10\n",
    "  month                 the month to query, example: 2\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --day [day]           the day to query, if given, only one day will be\n",
    "                        handled\n",
    "  --db [db_name]        the db name (name only), example: \"example.db\"\n",
    "  --cache [cache_dir]   the cache folder, example: data\n",
    "  --buffer [buffer_max]\n",
    "                        the max buffer size: the size of avro entries that are\n",
    "                        buffered in the memory\n",
    "\n",
    "```\n",
    "\n",
    "In order to lead to memory overflow, there is a \"buffer_size\" parameter that\n",
    "can limit the number of the records buffered inside the \"dao\". If the \"buffer_size\" is set to 0,\n",
    "which means the buffer size is not limited, then all of the records of \"avro-file\" will be loaded\n",
    "at once and save to the sql, so the speed will be faster than using a buffer size limitation.\n",
    "\n",
    "The extracting of data from \"avro-file\" is implemented with multiprocessing, each \"avro-file\" will be\n",
    "processed by one thread (process), so the speed can be fast.\n",
    "\n",
    "The downloading can similarly also be implemented with multiprocessing,\n",
    "but downloading too many files and extracting all \"avro-files\" may cost too much storage. In this reason,\n",
    "the software will clean all of the tar- and avro- files immediately after finished the extracting.\n",
    "\n",
    "The program provides 2 DAO(Data Access Object) files: \"dao\" and \"pandao\".\n",
    "\"dao\" uses the pure python \"sqlite3\" lib, and \"pandao\" uses the \"pandas\" lib."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# the program can also be run as a function call\n",
    "from question_b import handle_one_month\n",
    "\n",
    "# note: slow! about 10 min!\n",
    "handle_one_month(2020, 10, db_name=\"example.db\", buffer_max=10000)\n",
    "# will print: \"used time: 878.292760, total: 3682335\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Read up on the Route Views project to briefly answer the following questions: What kinds of data are Route\n",
    "Views collecting? What is a Routing Information Base (RIB) in this context? How are the collected RIBs different\n",
    "from BGP UPDATE data? (2 points)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "solution c):\n",
    "\n",
    "collecting real-time BGP information.\n",
    "\n",
    "RIB is a package or table like this:\n",
    "```\n",
    "TIME: 10/15/20 12:00:00\n",
    "TYPE: TABLE_DUMP_V2/IPV4_UNICAST\n",
    "PREFIX: 0.0.0.0/0\n",
    "SEQUENCE: 0\n",
    "FROM: 80.249.210.150 AS199524\n",
    "ORIGINATED: 08/14/20 13:35:07\n",
    "ORIGIN: IGP\n",
    "ASPATH: 199524 174\n",
    "NEXT_HOP: 80.249.210.150\n",
    "```\n",
    "It contains the AS path information (ASPATH), which shows\n",
    "the real path of a package.\n",
    "\n",
    "BGP UPDATE data only shows the accessibility of each BGP routers,\n",
    "but the RIB shows the real path of a package. BGP routers may use some strategies,\n",
    "they may prefer to send the packages to a \"far\" router based on the policies.\n",
    "So the RIB can show the real routing among AS."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "d) Write a program that adds columns for the AS numbers of the Autonomous Systems (ASes), which announce\n",
    "the IPv4 and IPv6 addresses of the resolved domains, to each table of your database. To map IP addresses to\n",
    "AS numbers (e.g., with pyasn\n",
    "), use BGP data collected from the Amsterdam Internet Exchange (AMS-IX),\n",
    "which you can download from the Route Views archive\n",
    ". For simplicity, it is sufficient to take one RIB file (e.g.,\n",
    "for the 15th of the month at 12:00 PM) for all IP-ASN-mappings over the whole month. Why do we choose data\n",
    "from AMS-IX rather than from other collectors? (2 points)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "solution d):\n",
    "\n",
    "Why do we choose data from AMS-IX rather than from other collectors?\n",
    "Because of the timezone.\n",
    "\n",
    "The program is \"question_d.py\".  To start it you should provide a multiprocessing environment and an existed database.\n",
    "Note: it is recommended to start the program with chunksize 1000\n",
    "\n",
    "An example is: **\"question_d.py alexa 2020 10 --chunksize 1000\"**\n",
    "\n",
    "```\n",
    "usage: question_d.py [-h] [--db [db_name]] [--cache [cache_dir]] [--chunksize [chunksize]] [--process [process]] source year month\n",
    "\n",
    "question_d\n",
    "\n",
    "positional arguments:\n",
    "  source                the source of the data, can be Alexa or Umbrella\n",
    "  year                  the year to query, example: 10\n",
    "  month                 the month to query, example: 2\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --db [db_name]        the db name (name only), example: \"example.db\"\n",
    "  --cache [cache_dir]   the cache folder, example: data\n",
    "  --chunksize [chunksize]\n",
    "                        the max chunksize to read from sql table every time\n",
    "  --process [process]   the number of process to read and update the sql table\n",
    "\n",
    "```\n",
    "\n",
    "The program will download the AMS-IX RIB archives with the giving year and month\n",
    "and convert it to IPASN databases, use it for looking up the as."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the program can also be run as a function call\n",
    "from question_d import Asn\n",
    "\n",
    "# default 50 processes\n",
    "asn = Asn(\"example.db\", 'alexa',year=2020, month=10, chunksize=1000, process_number=50)\n",
    "asn.flush_ases()\n",
    "# used time: 643.814716\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "e) Read up on different mechanisms with which CDNs (and similarly cloud providers) achieve client redirection7\n",
    ".\n",
    "Download the regexes.csv and asns.csv files from Moodle (note that the provided files will not exhaustively\n",
    "and correctly identify all cloud/CDN providers8\n",
    ", however, the accuraccy will be sufficient for this assignment).\n",
    "Since the AS numbers of the providers in the list were looked up with both RIPEstat9 and PeeringDB10, some\n",
    "provider-ASN-mapping might be duplicateâ€”you can simply drop such duplicates and only keep distinct tuples.\n",
    "Briefly describe how and why the information provided by these files can be used to identify cloud/CDN providers.\n",
    "Write a program that adds a column to each table of your database for the identified website host based on the\n",
    "two files and following criteria:\n",
    "1. If a CNAME for a domain exists, apply the regular expressions to potentially identify the website host.\n",
    "2. If no website host was found via CNAME, identify the website host via the AS numbers for IPv4 and IPv6.\n",
    "3. If neither CNAME nor ASN identified the website host, put -1 as the website host.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "solution e):\n",
    "\n",
    "The program is \"question_e.py\".  To start it you should provide a multiprocessing environment and an existed database.\n",
    "Note: it is recommended to start the program with chunksize 1000\n",
    "\n",
    "An example is: **\"question_d.py alexa --chunksize 1000\"**\n",
    "\n",
    "The options of \"question_e.py\" are similar with d)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the program can also be run as a function call\n",
    "from question_e import Host\n",
    "\n",
    "# default 50 processes\n",
    "h = Host(\"example.db\", 'alexa', chunksize=1000, process_number=50)\n",
    "h.flush_host()\n",
    "# used time: 841.023135"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "f) How many unique websites (absolute and relative count) does each website host (incl. -1, i.e., non-cloud and\n",
    "non-CDN hosts) serve over the whole month? Provide a CSV file (columns: host, num_uniq_websites)\n",
    "and sort the hosts in descending order. Does your observation support or contradict the perception described in\n",
    "the beginning of the assignment? Do you see significant differences between the Alexa and Umbrella data?\n",
    "(2 points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "solution f):\n",
    "\n",
    "The program is \"question_f.py\".\n",
    "For the csv, I use a subset of the data(data of the day: 20201015).\n",
    "But the program can use for the whole month.\n",
    "\n",
    "\n",
    "The csv files are 'Umbrella_out.csv' and 'Alexa_out.csv'\n",
    "\n",
    "The observe is the same with my perception: the most websites are small webs without CDN and the most CDNs are from the big cloud server companies.\n",
    "\n",
    "We can see that Cloudflare, Amazon are the most popular CDNs w.r.t. the Umbrella source,\n",
    "and most websites are without CDNs, and it is different from Alexa."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "g) Read up on different Internet toplists11 to briefly answer the following questions: Which different toplists\n",
    "exist? What is the difference between them? How are the lists generated? What can be said about their\n",
    "stability? (2 points)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alexa: http traffic, Umbrella: DNS /IPs, Majestic:backlinks, Quantcast: visit number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "h) Write a program that downloads and unpacks the respective Alexa and Umbrella toplists for the whole month\n",
    "from a toplist archive12. Consider that the naming scheme of the files in the archive changed in May 2018.\n",
    "Load the daily toplists into a dataframe or table each. Write a program to enrich these daily toplists with the\n",
    "website host column from your database by matching the domain names. Note that you may need to add\n",
    "the www. subdomain to the domains for a successful join. For each Alexa and Umbrella, save the resulting\n",
    "dataframes/tables with toplist rank, domain name, IPv4/IPv6, website host, and date as a new table in your SQL\n",
    "database. (2 points)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "solution h):\n",
    "\n",
    "The program is \"question_h.py\".\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}